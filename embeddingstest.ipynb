{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and initialize AOAI connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import PyPDF2\n",
    "import openai\n",
    "import requests\n",
    "import pandas as pd\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_EMBEDDING_KEY\") \n",
    "RESOURCE_ENDPOINT = os.getenv(\"AZURE_OPENAI_EMBEDDING_ENDPOINT\") \n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = API_KEY\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "openai.api_version = \"2022-12-01\"\n",
    "\n",
    "url = openai.api_base + \"/openai/deployments?api-version=2022-12-01\"\n",
    "\n",
    "r = requests.get(url, headers={\"api-key\": API_KEY})\n",
    "\n",
    "# print(r.text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the search engine and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_engine = 'text-search-davinci-doc-001'\n",
    "# search_engine = 'text-search-davinci-query-001'\n",
    "embeddings_engine = 'text-search-curie-doc-001' \n",
    "search_engine = 'text-search-curie-query-001'\n",
    "query_string = 'What are potential regulatory actions that can result from an inspection?' # 'what ports and connectors does my surface have?'\n",
    "summarization_engine = \"text-davinci-002\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in .pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_into_df(pdf_file_path):\n",
    "    # create a pdf file object\n",
    "    pdfFileObj = open(pdf_file_path, 'rb') # rb means read binary mode \n",
    "    # create a pdf reader object\n",
    "    pdfReader = PyPDF2.PdfReader(pdfFileObj) \n",
    "    # print the number of pages in pdf document \n",
    "    print('Number of pages in PDF: ', len(pdfReader.pages)) \n",
    "\n",
    "    # declare an empty dataframe with two columns: Page and Text (for page number and text of each page)  \n",
    "    df = pd.DataFrame(columns=['page', 'text'])\n",
    "\n",
    "    # loop through the number of pages in the document, get each page's text, write to a new row in dataframe  \n",
    "    for i in range(0, len(pdfReader.pages)):      # for each page \n",
    "\n",
    "        pageObj = pdfReader.pages[i]     # get that page object from the pdf reader object using indexing  \n",
    "\n",
    "        text = pageObj.extract_text()       # extract the text from that page object  \n",
    "\n",
    "        df_temp = pd.DataFrame([[i+1,text]], columns=['page', 'text'])     # create temporary dataframe to hold information for each iteration (page number and text on that page)  \n",
    "\n",
    "        # df = df.append(df_temp)            # append temporary dataframe to master dataframe at end of each iteration (loop through all pages of document)  \n",
    "        df = pd.concat([df, df_temp], ignore_index=True)     # append temporary dataframe to master dataframe at end of each iteration (loop through all pages of document)\n",
    "\n",
    "        print('Finished processing page %d' % (i+1))      # print statement so you can see what pages have been processed already while script is running \n",
    "\n",
    "    print('Finished processing all pages.')               # print statement when script has finished running \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in PDF:  66\n",
      "Finished processing page 1\n",
      "Finished processing page 2\n",
      "Finished processing page 3\n",
      "Finished processing page 4\n",
      "Finished processing page 5\n",
      "Finished processing page 6\n",
      "Finished processing page 7\n",
      "Finished processing page 8\n",
      "Finished processing page 9\n",
      "Finished processing page 10\n",
      "Finished processing page 11\n",
      "Finished processing page 12\n",
      "Finished processing page 13\n",
      "Finished processing page 14\n",
      "Finished processing page 15\n",
      "Finished processing page 16\n",
      "Finished processing page 17\n",
      "Finished processing page 18\n",
      "Finished processing page 19\n",
      "Finished processing page 20\n",
      "Finished processing page 21\n",
      "Finished processing page 22\n",
      "Finished processing page 23\n",
      "Finished processing page 24\n",
      "Finished processing page 25\n",
      "Finished processing page 26\n",
      "Finished processing page 27\n",
      "Finished processing page 28\n",
      "Finished processing page 29\n",
      "Finished processing page 30\n",
      "Finished processing page 31\n",
      "Finished processing page 32\n",
      "Finished processing page 33\n",
      "Finished processing page 34\n",
      "Finished processing page 35\n",
      "Finished processing page 36\n",
      "Finished processing page 37\n",
      "Finished processing page 38\n",
      "Finished processing page 39\n",
      "Finished processing page 40\n",
      "Finished processing page 41\n",
      "Finished processing page 42\n",
      "Finished processing page 43\n",
      "Finished processing page 44\n",
      "Finished processing page 45\n",
      "Finished processing page 46\n",
      "Finished processing page 47\n",
      "Finished processing page 48\n",
      "Finished processing page 49\n",
      "Finished processing page 50\n",
      "Finished processing page 51\n",
      "Finished processing page 52\n",
      "Finished processing page 53\n",
      "Finished processing page 54\n",
      "Finished processing page 55\n",
      "Finished processing page 56\n",
      "Finished processing page 57\n",
      "Finished processing page 58\n",
      "Finished processing page 59\n",
      "Finished processing page 60\n",
      "Finished processing page 61\n",
      "Finished processing page 62\n",
      "Finished processing page 63\n",
      "Finished processing page 64\n",
      "Finished processing page 65\n",
      "Finished processing page 66\n",
      "Finished processing all pages.\n"
     ]
    }
   ],
   "source": [
    "# Path to download the PDF to run this exercise: 'https://download.microsoft.com/download/7/B/1/7B10C82E-F520-4080-8516-5CF0D803EEE0/surface-book-user-guide-EN.pdf'\n",
    "# surface_user_guide_path = 'C:/Users/ianadams/OneDrive - Microsoft/surface-book-user-guide-EN.pdf'\n",
    "# surface_user_guide_df = read_pdf_into_df(surface_user_guide_path)\n",
    "\n",
    "fda_7348_810_path = \"C:/Users/ianadams/OneDrive - Microsoft/CP 7348.810 Sponsors and CROs_FINAL.pdf\"\n",
    "surface_user_guide_df = read_pdf_into_df(fda_7348_810_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the cover and contents pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = list(range(0,2)) # Drops Cover and Copyright\n",
    "# rows_to_drop = list(range(0,5)) # Drops Cover and Table of Contents\n",
    "surface_user_guide_df = surface_user_guide_df.drop(rows_to_drop, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Text in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s is input text\n",
    "def normalize_text(s, sep_token = \" \\n \"):\n",
    "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
    "    s = re.sub(r\". ,\",\"\",s)\n",
    "    # remove all instances of multiple spaces\n",
    "    s = s.replace(\"..\",\".\")\n",
    "    s = s.replace(\". .\",\".\")\n",
    "    s = s.replace(\"\\n\", \"\")\n",
    "    s = s.strip()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_user_guide_df['text'] = surface_user_guide_df['text'].apply(lambda x : normalize_text(x))\n",
    "# surface_user_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a token count for each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "surface_user_guide_df['n_tokens'] = surface_user_guide_df[\"text\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "# surface_user_df = surface_user_df[surface_user_df.n_tokens<2000]\n",
    "len(surface_user_guide_df)\n",
    "# surface_user_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the embeddings for each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_user_guide_df['davinci_search'] = surface_user_guide_df[\"text\"].apply(lambda x : get_embedding(x, engine = embeddings_engine))\n",
    "# surface_user_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our search and execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search through the reviews for a specific product\n",
    "def search_docs(df, user_query, search_engine, top_n=3, to_print=True):\n",
    "    embedding = get_embedding(\n",
    "        user_query,\n",
    "        engine=search_engine\n",
    "    )\n",
    "    df[\"similarities\"] = df.davinci_search.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = (\n",
    "        df.sort_values(\"similarities\", ascending=False)\n",
    "        .head(top_n)\n",
    "    )\n",
    "    if to_print == True:\n",
    "        display(res)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>davinci_search</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>7348.810 7348.810 Date of Issuance: 09/ 15/202...</td>\n",
       "      <td>581</td>\n",
       "      <td>[-0.006083174142986536, 0.0046507855877280235,...</td>\n",
       "      <td>0.373887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>59</td>\n",
       "      <td>7348.810 7348.810 Date of Issuance: 09/ 15/202...</td>\n",
       "      <td>522</td>\n",
       "      <td>[-0.00846640020608902, -0.0023893050383776426,...</td>\n",
       "      <td>0.371995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>7348.810 7348.810 Date of Issuance: 09/ 15/202...</td>\n",
       "      <td>497</td>\n",
       "      <td>[-0.014045056886970997, -0.008437714539468288,...</td>\n",
       "      <td>0.370509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>7348.810 7348.810 Date of Issuance: 09/ 15/202...</td>\n",
       "      <td>509</td>\n",
       "      <td>[-0.0002744222874753177, -0.00825151614844799,...</td>\n",
       "      <td>0.360411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page                                               text  n_tokens  \\\n",
       "14   15  7348.810 7348.810 Date of Issuance: 09/ 15/202...       581   \n",
       "58   59  7348.810 7348.810 Date of Issuance: 09/ 15/202...       522   \n",
       "59   60  7348.810 7348.810 Date of Issuance: 09/ 15/202...       497   \n",
       "16   17  7348.810 7348.810 Date of Issuance: 09/ 15/202...       509   \n",
       "\n",
       "                                       davinci_search  similarities  \n",
       "14  [-0.006083174142986536, 0.0046507855877280235,...      0.373887  \n",
       "58  [-0.00846640020608902, -0.0023893050383776426,...      0.371995  \n",
       "59  [-0.014045056886970997, -0.008437714539468288,...      0.370509  \n",
       "16  [-0.0002744222874753177, -0.00825151614844799,...      0.360411  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_results = search_docs(surface_user_guide_df, query_string, search_engine, top_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>page</th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>davinci_search</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>7348.810 7348.810 Date of Issuance: 09/ 15/202...</td>\n",
       "      <td>581</td>\n",
       "      <td>[-0.006083174142986536, 0.0046507855877280235,...</td>\n",
       "      <td>0.373887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>59</td>\n",
       "      <td>7348.810 7348.810 Date of Issuance: 09/ 15/202...</td>\n",
       "      <td>522</td>\n",
       "      <td>[-0.00846640020608902, -0.0023893050383776426,...</td>\n",
       "      <td>0.371995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>7348.810 7348.810 Date of Issuance: 09/ 15/202...</td>\n",
       "      <td>497</td>\n",
       "      <td>[-0.014045056886970997, -0.008437714539468288,...</td>\n",
       "      <td>0.370509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>7348.810 7348.810 Date of Issuance: 09/ 15/202...</td>\n",
       "      <td>509</td>\n",
       "      <td>[-0.0002744222874753177, -0.00825151614844799,...</td>\n",
       "      <td>0.360411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index page                                               text  n_tokens  \\\n",
       "0     14   15  7348.810 7348.810 Date of Issuance: 09/ 15/202...       581   \n",
       "1     58   59  7348.810 7348.810 Date of Issuance: 09/ 15/202...       522   \n",
       "2     59   60  7348.810 7348.810 Date of Issuance: 09/ 15/202...       497   \n",
       "3     16   17  7348.810 7348.810 Date of Issuance: 09/ 15/202...       509   \n",
       "\n",
       "                                      davinci_search  similarities  \n",
       "0  [-0.006083174142986536, 0.0046507855877280235,...      0.373887  \n",
       "1  [-0.00846640020608902, -0.0023893050383776426,...      0.371995  \n",
       "2  [-0.014045056886970997, -0.008437714539468288,...      0.370509  \n",
       "3  [-0.0002744222874753177, -0.00825151614844799,...      0.360411  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results.reset_index(inplace=True)\n",
    "search_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass text from top 3 results to AOAI for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_engineering = 'You must summarize the results of the ----SEARCH RESULTS---- section in a way that best answers the query listed in the ----USER QUERY--- section \\n \\n'\n",
    "def create_final_prompt(df, user_query, prompt_engineering):\n",
    "    res = prompt_engineering + '----USER QUERY----\\n' + user_query + '\\n\\n' + '----SEARCH RESULTS----\\n'\n",
    "\n",
    "    for i in range(3):\n",
    "        res += df['text'][i] + '\\n'\n",
    "    \n",
    "    print(res)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must summarize the results of the ----SEARCH RESULTS---- section in a way that best answers the query listed in the ----USER QUERY--- section \n",
      " \n",
      "----USER QUERY----\n",
      "What are potential regulatory actions that can result from an inspection?\n",
      "\n",
      "----SEARCH RESULTS----\n",
      "7348.810 7348.810 Date of Issuance: 09/ 15/2021 Page 15 of 66 FORM FDA 2438g (electronic -09/2003) PART III - INSPECTIONAL The primary focus of sponsor inspections is to evaluate the sponsor’s practices and procedures to determine compliance with applicable regulations and adherence to good clinical practice standards to ensure subject protection and data quality and integrity. These inspections may include, but are not limited to, a review of the sponsor’s practices and procedures related to clinical trial oversigh including activities such as site monitoring, vendor audit training, and data collection, handling, and management. The inspectional focus is not to scientifically evaluate the results of the study or the quality of the protocol. A. GENERAL The following instructions apply to all sponsor inspections: 1. Sponsor inspections are product- type specific (i.e., huma n drugs, biologic al product and devices; animal drugs; animal feed ; foods, food additives, and color additive s; and tobacco products). Apply the applicable regulations accordingly to each sponsor inspection. 2. Inspections under this program may be pre-announced unless otherwise instructed in the inspection assignment and at the discretion of the ORA O BIMO division . Pre-announcement should generally be no less than 5 calendar days in advance of the inspection. Inspections may be conducted sooner than 5 c alendar days if requested by or acceptable to the establishment and if this date is acceptable to the ORA investiga tor. The ORA investigator should immediately report to their supervisor, DI and the center POC, any attempt by the sponsor to delay an insp ection without sufficient justification. Refer to IOM Chapter 5, Establishment Inspections for further instruction. For inspections conducted at military installations, the ORA investigator should contact the Chief of Professional Services at the facility to be inspected. If the inspection involves a U.S. Depar tment of Veterans Affairs (VA) facility, refer to Part II Section B.6. (Inspections of Facilities under the Jurisdiction of the Department of Veterans Affairs) of this CP for additional instructions. 3. If the ORA investigator encounters a refusal to permit entry or inspection, or a refusal of information, including a refusal to permit access or copying of requested records, consult IOM Chapter  Establishment Inspection and applicable statutory and regulatory requirements8 and follow current policy/procedures. 4. If any observations or suspected observations of regulatory or statutory deviations affect data 8 Sections 301(f) and 704 of the Federal Food, Drug and Cosmetic Act ( FD&C Act), Sections 351(c), 360A(a), (b) & (f); 360B(a); and 361(a) of the Public Health Service (PHS) Act, and 21 CFR 312.68 and 812.145\n",
      "7348.810 7348.810 Date of Issuance: 09/ 15/2021 Page 59 of 66 FORM FDA 2438g (electronic -09/2003) PART V - REGULATORY/ADMINISTRATIVE STRATEGY The follow ing information is to be used in conjunction with the instructions in FMD -86 (Establishment Inspection Report Conclusions and Decisions) for initial ORA O BIMO division and final center classification of inspections under this CP: No Action Indicated (NAI) – No objectionable conditions or practices were found during an inspection or the significance of any objectionable conditions found does not justify further regulatory action. Voluntary Action Indic ated (VAI) – Objectionable conditions or practices were found, but FDA is not prepared to take or recommend any regulatory action since the objectionable conditions or practices do not meet the threshold for regulatory action. Official Action Indicated (OAI) – Objectionable conditions and/or practices were fou nd, and regulatory action should be recommended. The scope, severity, or pattern of the violation(s) support findings that: 1. Subjects under the care of the sponsor would be or have been exposed to an unreasonable and significant risk of illness or injury; or 2. Subjects' rights, welfare, or safety would be or have been seriously compromised; or 3. Data integrity or reliability is or has been compromised. The ORA O BIMO division should consult with the center POC when an OAI classification is recommended to allow for discussion of the recommendation. The center is responsible for the final classification of inspections. The center is responsible for drafting, developing, and issuing all regulatory and enforcement letters. Post -inspectional correspondence for VAI inspections may identify significant issues and, when needed, state that FDA expects prompt, voluntary corrective action by the sponsor. Post- inspectional correspondence for NAI inspections issued by the center may indicate that no objectionable conditions or practices were identified that would justify enforcement action. Advisory, administrative, and judicial actions may be pursued based on the inspectional observations and will be in accordance wit h federal laws and regulations . FDA can invoke legal san ctions under the Federal Food Drug and Cosmetic Act ( FD&C Act ) and/or Title 18, U.S.C. where appropriate. FDA may pursue the following based on inspectional observations: 1. An Untitled Letter (UL) cites violations that do not meet the threshold of regulatory significance for a Warning Letter. 2. A Warning Letter may be considered when the violations can be corrected through specific\n",
      "7348.810 7348.810 Date of Issuance: 09/ 15/2021 Page 60 of 66 FORM FDA 2438g (electronic -09/2003) action(s) by the sponsor (e.g., preparation of, and compliance with, a deta iled corrective action plan that is acceptable to FDA) and adherence to the corrective action plan has a high probability of preventing similar or other violations from occurring in the future. 3. Rejection of dat a 4. Seizure of investigational product 5. Injunctio n 6. Civil Money Penalties 7. Prosecution under the FD&C Act or other Federal statutes ( e.g., 18 U.S.C. 2, 371, 1001, and 1341) . The Agency may also pursue other necessary actions (e.g., consent agreements, follow -up inspections, clinical hold for studies subjec t to 21 CFR 312, terminate the investigational new animal drug (INAD) exemption subject to 21 CFR 511.1(d), refuse to approve or withdraw the approval of new animal drug application ( NADA ) subject to 21 CFR 514, terminate the exemption or withdrawal of approval of IDE application for st udies subject to 21 CFR 812, regulatory meeting or device detention based on the inspectional observations ). See FMD -86 and Regulatory Procedures Manual (RPM) . Referral of pertinent matters may be mad with center concurrence, to other federal, state, or local agencies for such action as that agency dee ms appropriate. For sponsor -investigators, additional administrative/enforcement actions that may be applicable are described in the Clinical Investigators and Sponsor- Investigators Compliance Program (CP 7348.811) . Follow -up Inspections: 1. ORA OBIMO division follow -up actions, including re -inspection, will be made at the request of the center. Centers should evaluate whether the violations found indicate systemic proble ms with the conduct of the study or the reliability of the data and whether additional inspection assignments should be issued. 2. Following issuance of a Warning Letter, c enters should periodically review their databases for entrie including whether a War ning Letter recipient is ac tively conducting other clinical investigations. If such information becomes available, the center should issue a follow- up inspection assignment to verify if the sponsor is fulfilling the terms of any corrective action plans and in compliance with applic able regulations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_prompt = create_final_prompt(search_results, query_string, prompt_engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_text_to_final_response(prompt, summarization_engine):\n",
    "    # Passes the response from the question answering bot to the AOAI model\n",
    "\n",
    "    # Format the output from the QA bot to include the signifier for a summarization from AOAI\n",
    "    # bot_answer = answer + '\\n\\nTl;dr'\n",
    "\n",
    "    # REVIEW: Print the length of the bot answer for reference - may want to cut off since summarization\n",
    "    # doesn't work well with short answer.\n",
    "    # print(len(bot_answer))\n",
    "\n",
    "    # Submit the answer from the QA Bot to the AOAI model for summariation\n",
    "    response = openai.Completion.create(\n",
    "      engine=summarization_engine,\n",
    "      prompt= prompt, \n",
    "      temperature=0.7,\n",
    "      max_tokens=2048,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=None)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = results_text_to_final_response(final_prompt, summarization_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Azure Open AI response:  7348\n",
      "\n",
      "The FDA can take a variety of regulatory actions as a result of an inspection, including but not limited to issuing an Untitled Letter, a Warning Letter, rejecting data, seizing investigational product, issuing an injunction, pursuing civil or criminal prosecution, or imposing civil money penalties.\n"
     ]
    }
   ],
   "source": [
    "def print_formatted_response(open_ai_response):\n",
    "        print(\"\\n\", \"Azure Open AI response: \", open_ai_response['choices'][0]['text'])\n",
    "\n",
    "print_formatted_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
